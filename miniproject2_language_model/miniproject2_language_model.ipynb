{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--Cvru1cgwyP"
      },
      "source": [
        "# **Miniproject 2**\n",
        "## **~Large~ Small Language Model**\n",
        "\n",
        "### **Objective**\n",
        "Implement a transformer-based, character-level language model (GPT-like) and train it on the Shakespeare dataset. By the end of this project, you should be able to generate Shakespearean-like text given a seed string.\n",
        "\n",
        "You will probably want to train the model on a GPU. You can use free GPUs on [Google Colab](https://colab.research.google.com/?utm_source=scs-index)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_rT3xwrhieb"
      },
      "source": [
        "### **Dataset**:\n",
        "\n",
        "The Shakespeare dataset contains the complete works of William Shakespeare, including his plays, poems, and sonnets.\n",
        "\n",
        "[**Download link**](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt)\n",
        "\n",
        "In a character-level language model, each character in the input data is mapped to its respective index from a dictionary. The input to the model is in the form (B, N), where B is the batch size and N is the number of tokens for each sequence. The model was tested with B=N=128, but feel free to explore different values.\n",
        "\n",
        "An interface for the dataset class that takes care of tokenization is provided below.\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CharDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Emits batches of characters.\n",
        "\n",
        "    Adapted from \"https://github.com/karpathy/minGPT\".\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, data):\n",
        "\n",
        "        chars = ... # get characters from the input data\n",
        "        self.stoi = { ch:i for i,ch in enumerate(chars) } # map characters to integer indices\n",
        "\n",
        "        ...\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def __len__(self):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # grab a chunk of (block_size + 1) characters from the data\n",
        "        # encode every character to an integer\n",
        "        # return the chunk and the shifted version as tensors\n",
        "        pass\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VV7OAXGRhf_V"
      },
      "source": [
        "### **Requirements**\n",
        "\n",
        "#### **Architecture**\n",
        "\n",
        "Implement the Transformer's decoder-only structure.\n",
        "This includes\n",
        "\n",
        "* input token embeddings\n",
        "* the causal multi-head self-attention mechanism\n",
        "* feed-forward neural networks\n",
        "* positional encodings, residual connections, layer normalizations.\n",
        "\n",
        "The project was tested with $12$ layers, $8$ attention heads, and $768$ embedding dimensions, on a single GPU.\n",
        "\n",
        "The `forward` method for the entire model has the following form:\n",
        "\n",
        "```\n",
        "tok_emb = WTE(idx) # token embeddings\n",
        "pos_emb = WPE(pos) # position embeddings\n",
        "x = Dropout(tok_emb + pos_emb)\n",
        "for Block in Blocks:\n",
        "    x = Block(x)\n",
        "x = Final_LayerNorm(x)\n",
        "logits = LM_Head(x)\n",
        "```\n",
        "\n",
        "The `forward` method for the transformer block has the following form:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "x = x + self.CausalSelfAttn(self.LayerNorm_1(x))\n",
        "out = x + self.MLP(self.LayerNorm_2(x))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### **Training**\n",
        "\n",
        "In a character-level transformer language model, the goal is to predict the next character in a sequence given the previous characters. To train such a model effectively, we use two versions of our data: the input sequence and a shifted version of this sequence, which serves as the target for our predictions.\n",
        "\n",
        "Preprocess the dataset to a character-level representation.\n",
        "Use a sliding window approach for sequence chunks (e.g., window size of $128$ characters).\n",
        "Implement causal masking for the self-attention mechanism.\n",
        "Use the [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) optimizer and the cross-entropy loss.\n",
        "\n",
        "**Optional**:\n",
        "\n",
        "* Implement a learning rate decay strategy\n",
        "* Implement gradient clipping\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#### **Evaluation and Inference**\n",
        "\n",
        "* Monitor the cross-entropy loss. Use a seed string to initialize the model and generate Shakespearean-like text.\n",
        "\n",
        "* In order to generate the characters, at each generation step you can either select the character with the highest probability, or you can sample according to the output distribution.\n",
        "\n",
        "The high-level pseudocode for generation is:\n",
        "\n",
        "```python\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    context = \"O God, O God!\"\n",
        "    tokenized_context = tokenize(context)\n",
        "    # the model should implement a method to generate tokens given a prompt\n",
        "    y = model.generate(tokenized, ...)\n",
        "    completion = tokens_to_string(y)\n",
        "```\n",
        "\n",
        "**Optional**:\n",
        "* Compute the [perplexity](https://medium.com/@priyankads/perplexity-of-language-models-41160427ed72#:~:text=Intuitively%2C%20perplexity%20means%20to%20be,loss%20obtained%20from%20the%20model.) metric for quantitative evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8t88Dcn8JZ8M"
      },
      "source": [
        "### **Example Outputs**\n",
        "\n",
        "The following are my outputs after $6000$ steps of training, with the seed string \"O God, O God!\"\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "O God, O God! neither? unto the base very ears,\n",
        "As damned with it.\n",
        "\n",
        "DUKE OF YORK:\n",
        "Away! Once more, one word.\n",
        "\n",
        "RICHARD:\n",
        "Clove, dear so; and therein my son will be\n",
        "false of woe: if ye seems to be the mother\n",
        "Of gracious order this time when R going kinsperse eyes,\n",
        "What dost bewreck her fairer drying tears.\n",
        "\n",
        "NORTHUMBERLAND:\n",
        "Have you forgot the Duke of Norfolk, get him to\n",
        "again; and and agilic: there is my spirit\n",
        "So maly did must such a marble perfection.\n",
        "\n",
        "ELBOW:\n",
        "Come, bring them with oaths, and so deliver\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0SY7CGAhnkp"
      },
      "source": [
        "### Resources:\n",
        "\n",
        "* Vaswani et al., \"Attention is All You Need\": [link](https://arxiv.org/abs/1706.03762)\n",
        "\n",
        "* Illustrated Transformer by Jay Alammar: [link](https://jalammar.github.io/illustrated-transformer/)\n",
        "\n",
        "* OpenAI GPT-2 Paper: [link](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
        "\n",
        "* Deep Learning Course slides on transformers: [link](https://fleuret.org/dlc/materials/dlc-handout-13-3-transformers.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aD_GTMlZLgpp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dZdSRWPmgt-H"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    def __init__(self):\n",
        "        self.block = 128\n",
        "        self.batch = 128\n",
        "        self.epochs = 15\n",
        "        self.num_heads = 8\n",
        "        self.lr = 0.00008\n",
        "        self.embed_dim = 768\n",
        "        self.num_layers = 12\n",
        "        self.dropout = 0.1\n",
        "        self.seed = 42\n",
        "        self.device = torch.device(\"cuda\") #if torch.cuda.is_available() else \"cpu\"\n",
        "        self.data_size = 0\n",
        "        self.vocab_size = 0\n",
        "\n",
        "config = Config()\n",
        "\n",
        "class CharDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Emits batches of tokens.\n",
        "\n",
        "    Adapted from \"https://github.com/karpathy/minGPT\".\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, data):\n",
        "        self.config = config\n",
        "\n",
        "\n",
        "        tokens =  sorted(list(set(data))) # get tokens from the input data\n",
        "        self.stoi =  { ch:i for i,ch in enumerate(tokens) } # map tokens to integer indices\n",
        "        self.itos =  { i:ch for i,ch in enumerate(tokens) } # map integer indices to tokens\n",
        "        self.data = data\n",
        "        self.data_size = len(data)\n",
        "        self.vocab_size = len(tokens)\n",
        "\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        return self.vocab_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) // self.config.block\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # grab a chunk of (block_size + 1) tokens from the data\n",
        "        # encode every token to an integer\n",
        "        # return the chunk and the shifted version as tensors\n",
        "        chunk = self.data[idx * self.config.block : (idx + 1) * self.config.block + 1]\n",
        "        dix = [self.stoi[s] for s in chunk]\n",
        "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M71ZqaobBwk5",
        "outputId": "85c60e55-87f9-4db9-997d-760953e141ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size: 8714\n",
            "Vocab size: 65\n"
          ]
        }
      ],
      "source": [
        "text = open(\"input.txt\").read()\n",
        "dataset = CharDataset(config, text)\n",
        "\n",
        "print(f\"Dataset size: {len(dataset)}\")\n",
        "print(f\"Vocab size: {dataset.get_vocab_size()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nr2dQPcIMpb"
      },
      "source": [
        "<!-- multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
        "attn_output, attn_output_weights = multihead_attn(query, key,value) -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TOnIdrCEZUjQ"
      },
      "outputs": [],
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, dropout):\n",
        "        super(CausalSelfAttention, self).__init__()\n",
        "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len_0 = x.size(0)\n",
        "        # print(\"Sequence Length:\", seq_len)\n",
        "        self.causal_mask = torch.triu(torch.ones(seq_len_0, seq_len_0), diagonal=1).bool().to(x.device)\n",
        "\n",
        "        attn_output, _ = self.attention(x, x, x, attn_mask=self.causal_mask)\n",
        "        return attn_output #self.dropout(attn_output)\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_hidden_dim, dropout):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.ca = CausalSelfAttention(embed_dim, num_heads, dropout)\n",
        "        self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_dim, ff_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(ff_hidden_dim, embed_dim)\n",
        "        )\n",
        "        self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Self-attention with residual connection\n",
        "        x = x + self.ca(self.layer_norm1(x))\n",
        "\n",
        "        # Feed-forward with residual connection\n",
        "        x = x + self.feed_forward(self.layer_norm2(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "class TransformerLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, block_size, dropout=0.1):\n",
        "        super(TransformerLanguageModel, self).__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.position_embedding = nn.Embedding(block_size, embed_dim)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, embed_dim * 4, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.final_layer_norm  = nn.LayerNorm(embed_dim)\n",
        "        self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def forward(self, idx):\n",
        "        # Embedding and position embedding\n",
        "        tok_emb = self.token_embedding(idx)\n",
        "        pos = torch.arange(idx.size(1), device=idx.device).unsqueeze(0)\n",
        "        pos_emb = self.position_embedding(pos)\n",
        "\n",
        "        # Adding positional information\n",
        "        x = self.dropout(tok_emb + pos_emb)\n",
        "\n",
        "        # Pass through transformer blocks\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        # Output layer normalization and language model head\n",
        "        x = self.final_layer_norm(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    # def generate(self, tokenized_context, max_len, temperature=0.8,  sampling=False, top_k=None):\n",
        "    #     \"\"\"\n",
        "    #     Generate text from a trained model using successive blocks of size `block_size`.\n",
        "\n",
        "    #     Parameters:\n",
        "    #     - context: Seed string for generation.\n",
        "    #     - max_len: Total number of tokens to generate.\n",
        "    #     - sampling: If True, sample the next token based on probabilities. Otherwise, use the highest probability.\n",
        "    #     - top_k: If not None, only consider the top k most probable tokens for sampling.\n",
        "\n",
        "    #     Returns:\n",
        "    #     - Generated indices\n",
        "    #     \"\"\"\n",
        "    #     device = next(self.parameters()).device\n",
        "    #     generated = tokenized_context\n",
        "    #     context_tensor = torch.tensor(tokenized_context, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "\n",
        "    #     for _ in range(max_len):\n",
        "    #         # Crop the context to the last `block_size` tokens if it exceeds the limit\n",
        "    #         if context_tensor.size(1) > self.block_size:\n",
        "    #             context_tensor = context_tensor[:, -self.block_size:]\n",
        "\n",
        "    #         # Forward pass to get logits\n",
        "    #         logits = self(context_tensor)\n",
        "    #         logits = logits[:, -1, :]  # Take logits for the last token only\n",
        "    #         logits = logits / temperature  # Apply temperature scaling\n",
        "\n",
        "    #         if top_k is not None:\n",
        "    #             # Apply top-k sampling\n",
        "    #             top_k_values, top_k_indices = torch.topk(logits, k=top_k)\n",
        "    #             probs = torch.nn.functional.softmax(top_k_values, dim=-1)\n",
        "    #             next_token = torch.multinomial(probs, num_samples=1).item()\n",
        "    #             next_token_index = top_k_indices[next_token]\n",
        "    #             next_token_tensor = torch.tensor([[next_token_index]], device=device)\n",
        "    #             context_tensor = torch.cat((context_tensor, next_token_tensor), dim=1)\n",
        "    #             continue\n",
        "\n",
        "    #         probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "\n",
        "    #         # Decide the next token\n",
        "    #         if sampling:\n",
        "    #             next_token = torch.multinomial(probs, num_samples=1).item()\n",
        "    #         else:\n",
        "    #             next_token = torch.argmax(probs).item()\n",
        "\n",
        "    #         # Add the next token to the sequence\n",
        "    #         generated.append(next_token)\n",
        "    #         next_token_tensor = torch.tensor([[next_token]], device=device)\n",
        "    #         context_tensor = torch.cat((context_tensor, next_token_tensor), dim=1)\n",
        "\n",
        "    #         # Stop if we've generated enough tokens\n",
        "    #         if context_tensor.size(1) >= max_len:\n",
        "    #             break\n",
        "\n",
        "    #     return torch.tensor(generated)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, tokenized_context, max_new_tokens, temperature=1.0, do_sample=False, top_k=None):\n",
        "        \"\"\"\n",
        "        Generate text from a trained model using token indices.\n",
        "\n",
        "        Parameters:\n",
        "        - tokenized_context: LongTensor of shape (1, t), initial context (sequence of indices).\n",
        "        - max_new_tokens: Number of tokens to generate.\n",
        "        - temperature: Controls randomness in sampling; lower values make predictions more deterministic.\n",
        "        - do_sample: If True, sample from the probability distribution; otherwise, use greedy decoding.\n",
        "        - top_k: If not None, restrict sampling to top-k most probable tokens.\n",
        "\n",
        "        Returns:\n",
        "        - LongTensor of shape (1, t + max_new_tokens) with the generated indices.\n",
        "        \"\"\"\n",
        "        device = next(self.parameters()).device  # Get the model's device\n",
        "        idx = torch.tensor(tokenized_context, dtype=torch.long).unsqueeze(0).to(device)  # Shape (1, t)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Crop the context to the last block_size tokens if necessary\n",
        "            idx_cond = idx if idx.size(1) <= self.block_size else idx[:, -self.block_size:]\n",
        "\n",
        "            # Forward pass to get logits\n",
        "            logits = self(idx_cond)  # Shape (1, seq_len, vocab_size)\n",
        "            logits = logits[:, -1, :] / temperature  # Focus on the last token and apply temperature scaling\n",
        "\n",
        "            # Optionally apply top-k filtering\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, top_k, dim=-1)\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')  # Mask probabilities outside the top-k\n",
        "\n",
        "            # Convert logits to probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            # Decide the next token\n",
        "            if do_sample:\n",
        "                idx_next = torch.multinomial(probs, num_samples=1)  # Sample from probabilities\n",
        "            else:\n",
        "                _, idx_next = torch.topk(probs, k=1, dim=-1)  # Greedy decoding (most probable token)\n",
        "\n",
        "            # Append the predicted token to the sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1)  # Update the sequence with the new token\n",
        "\n",
        "        return  idx.squeeze(0)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsgeCbZhZUjR",
        "outputId": "fc186fc1-99d7-4e5b-833c-97b7ebc8cdc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15, Loss: 2.6655\n",
            "Epoch 2/15, Loss: 2.4884\n",
            "Epoch 3/15, Loss: 2.4768\n",
            "Epoch 4/15, Loss: 2.4711\n",
            "Epoch 5/15, Loss: 2.4668\n",
            "Epoch 6/15, Loss: 2.4642\n",
            "Epoch 7/15, Loss: 2.4621\n",
            "Epoch 8/15, Loss: 2.4602\n",
            "Epoch 9/15, Loss: 2.4586\n",
            "Epoch 10/15, Loss: 2.4568\n",
            "Epoch 11/15, Loss: 2.4566\n",
            "Epoch 12/15, Loss: 2.4546\n",
            "Epoch 13/15, Loss: 2.4533\n",
            "Epoch 14/15, Loss: 2.4524\n",
            "Epoch 15/15, Loss: 2.4515\n"
          ]
        }
      ],
      "source": [
        "#\n",
        "dataloader = DataLoader(dataset, batch_size=config.batch, shuffle=True)\n",
        "\n",
        "#\n",
        "model = TransformerLanguageModel(\n",
        "    vocab_size=dataset.get_vocab_size(),\n",
        "    embed_dim=config.embed_dim,\n",
        "    num_heads=config.num_heads,\n",
        "    num_layers=config.num_layers,\n",
        "    block_size=config.block,\n",
        "    dropout=config.dropout\n",
        ").to(config.device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "avg_loss = []\n",
        "# Training loop\n",
        "for epoch in range(config.epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for x, y in dataloader :\n",
        "\n",
        "        x = x.to(config.device) # Move x to the device\n",
        "        y = y.to(config.device) # Move y to the device\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(x)\n",
        "\n",
        "        #\n",
        "        loss = loss_fn(logits.view(-1, logits.size(-1)), y.view(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        #\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Compute avg loss\n",
        "    avg_loss.append(total_loss / len(dataloader))\n",
        "    print(f\"Epoch {epoch + 1}/{config.epochs}, Loss: {(total_loss / len(dataloader)):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "Ichb_hisM8wQ",
        "outputId": "b9bae980-bef2-42a8-dd16-b10564c805e4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#plotting the avg loss\n",
        "plt.plot(avg_loss)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Average Loss')\n",
        "plt.title('Average Loss per Epoch')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "X8BvRnoPenuo"
      },
      "outputs": [],
      "source": [
        "def tokens_to_string(tokens, itos):\n",
        "    \"\"\"\n",
        "    Convert a sequence of tokens to a string.\n",
        "\n",
        "    Args:\n",
        "    - tokens (torch.Tensor): Sequence of tokens.\n",
        "    - itos (dict): Integer-to-string mapping.\n",
        "\n",
        "    Returns:\n",
        "    - String representation of the tokens.\n",
        "    \"\"\"\n",
        "    return ''.join([itos[idx] for idx in tokens.tolist()])\n",
        "\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    Convert a string to a list of token indices.\n",
        "\n",
        "    Parameters:\n",
        "    - text: String to be tokenized.\n",
        "\n",
        "    Returns:\n",
        "    - List of token indices.\n",
        "    \"\"\"\n",
        "    return [dataset.stoi[c] for c in text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0jOqaelyeXIz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b9aa40f-cea1-4e4b-e602-474fea41755a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text:\n",
            "\n",
            "O God, O God!\n",
            "WI t,\n",
            "HAnd baveno tot od mardourerencon, tire id hamot weth ay derithinet ad my blicatha may?\n",
            "\n",
            "My t t w fa diro ou hed se whe he w or d men w sthell forars and, s te me ane bouro malore m mar s ancid ser hange by an ore m ay d d ate, hone fr thelowis hedssese st our hee s deanon anoro h beron,\n",
            "Hooulowno trot wes f wisasinon ate sinsssulim ou he m hor omord sas hinonousound d se, wear hendithee artarthan, t ome st fr st de s hodeses toullthedod,\n",
            "Taver f has the t a wind w ator ou bu by d h ainthieloore tomugesathat wed wisatand arethanore alaran thens,\n",
            "\n",
            "\n",
            "APELAns s w hithe d f t the t ou t t aras t se dead.\n",
            "Bendo d horingisinoun henore aile h baveea d hat.\n",
            "Seresouno wor shere avon f w torerd s a m ayeanor d bly! ooor t sor,\n",
            "CI hof t thr aroft ararince m ho ht tithy ha ourery bofathe th wis wioure hou th, m w tr blle me tisthang hondo f wincot wid ade or t houl be t, my\n",
            "ARDWh, therer f big me beaid a mars ho tho thengain ses myof thy hinsinont t d\n",
            "Thound faray mateaver sort my, we d t h, tant thar s thowe hyof onghintofonod.\n",
            "\n",
            "IUSedodomu st wiree, tered,\n",
            "\n",
            "Anthe w thathaves avel w avesthe d byof t bour d,\n",
            "Berarenonou oreat bearorstheartier ounofan,\n",
            "Cat fofoust shor sh, than wofthily heat ans, adanchous mone an, ord ssal wathimald d, d h woure se s athinoryo mis thy sononeaveelt t ouned s hay thanste bunon omath d myom f w toulate,\n",
            "Mor, tearorotot whan, h ma avereal faind om this sadimanongourysoueren sthayoryongengid, hedod\n",
            "CENI sthans, wnong sers her, th t her ste bes d thoftof thed,\n",
            "\n",
            "Whisthe she sono hit, wrde huginort\n",
            "Wasid avingison,\n",
            "Thor thesatat dasous merat he das hant, tes he t t t, d he haisand o ssshoues hes t aveedenoneshioo d t,\n",
            "T:\n",
            "Th,\n",
            "\n",
            "MAMA:\n",
            "IOLAnder bandome wer thy atofeal d foonathad f wod alou t hir, d t\n",
            "H:\n",
            "ATRERAne, aid, aly st mave be thy,\n",
            "IS:\n",
            "\n",
            "My aresat sul d mades dimime alitoowe allsout.\n",
            "A: that st.\n",
            "ARICI sind als hula wor s t ave f sistindethorer s the st mood\n",
            "ARD:\n",
            "METELUCO:\n",
            "ANELI'r beainon we d\n",
            "S:\n",
            "That manthe t herid be bus homerserearendeaithals stharinss th hat fend, beave d ath have w ha m ore d.\n",
            "Th me ate he ara he or, t o s a ad s wired t matishis matis thatistist hesonofothodsthange my are hed hithinan hathor t f blis onomaror at d,\n",
            "\n",
            "Beares tort, d ar bl meangerofireth be het t.\n",
            "Myor by\n",
            "By d at hiseesthetoughath hou borarer ow stha theser theend fur ainomy, ssthior d fthor wing hengen,\n",
            "AThay anee or t mbusto asit\n",
            "\n",
            "BINo wenthedies thot sheeanofe he t be hou s h f hy ay ou ws, an m de fir,\n",
            "T:\n",
            "\n",
            "\n",
            "I hit.\n",
            "ICand thas bard fr then and moul atrthe, my haringenom f barand.\n",
            "A:\n",
            "\n",
            "Anghenoronceras d bid tatheerde hond t,\n",
            "WAn witend aravease bucay.\n",
            "Anorsad\n",
            "\n",
            "\n",
            "WAMy he the se wis t thitod\n",
            "MIOngatofasse s mus str sind weart sthat he t tin heat d st hear todere win, ano w mothavas s o ar t t, thothofre d me d wn ald s trd othod tho se, ooror thatind,\n",
            "CHowor tend arashandsincanouseane atatrtoma ar houncle benge ththe t oon, fowow t we m me a or s till bl at of winou shansser ditomeseaild:\n",
            "Thangod theanod hean t thareanand.\n",
            "Thiside th t w frsth shy\n",
            "Warese thone wis torat s sugaro od thofade t orenofl winerath ar fry ton thare ar mer ond hy mainco sery, he h m dso ay thtr t.\n",
            "Ase t hanotond ou ters mesthe t bur sin m ho tot ou bes somer sta sowall wan, wereesig a aris moorangssimatha f homir avy?\n",
            "\n",
            "THARA:\n",
            "MI t heshore thethe arst tis thond be,\n",
            "ANo hinon athang w at d orave, wnd\n",
            "\n",
            "\n",
            "Thit se horary f thotait haig ar my bll t s, ongis the m tit ad od fr,\n",
            "Carond st t, thoushowill tomatheralore f onds hie musthy winger sheat shous, med.\n",
            "I stry?\n",
            "\n",
            "Whed:\n",
            "I: and\n",
            "\n",
            "Werde by bl oree,\n",
            "Se hener ouse thy s al are my,-mind, d s,\n",
            "CANot s wn fr s w athis wes s wod:\n",
            "Theavo t as,\n",
            "I'terer he marar sho hite,\n",
            "Th felat wn oun avencot head werdarartesit her ash me we andooure be thil hye st by be,\n",
            "AMI t, t hean o d.\n",
            "TI:\n",
            "Thange tar ance s he dancor,\n",
            "MPOflay t,\n",
            "TRUELoroun and.\n",
            "\n",
            "Th t s bane d throucour,\n",
            "WI sindsed. t\n",
            "We th ha f ousho an bar denont we tal thethand t\n",
            "IO:\n",
            "\n",
            "\n",
            "HARETRAMo s, by\n",
            "An hesin has onou wind thyom fin, s t are,\n",
            "I' thtenor sond\n",
            "MAneesthe me hore are d he s aser be s fan ay tho d thave ansiris seerd s mist tist hartot oncung d hot thallal s bleashatoo tourd ar d hravediteerensthel of thay theainen whe t send,\n",
            "\n",
            "Tirilord heng browind f h my f wellenon t h tand fl hinonghand w t h t, thisheas fe owo adeno wistondas f anorir wilar hy h are sen tr h haselor shadonorye and ord ore hordothoond tir thig dord ha th d, bistase, orsurd, omas tho trirsthomasha hid f arir saneas weat by d ay wim thy me thistofat then, t s thid tho beas bath t,\n",
            "Ant\n",
            "\n",
            "\n",
            "\n",
            "Myo f the d, s m ser athes f ma at, h ay seathotoneend m tingod arellad hore wrans brthe bld,\n",
            "Thave thars t aimy:\n",
            "Anousonot d my\n",
            "\n",
            "ARARD: fans thand heea t,\n",
            "The thy, dour andigeave bulthend tho ad be tind\n",
            "BEdon, t, havinginedean wedatorentho hys te fon we at wil saimyouseashe fardser be st bowre sar, or,\n",
            "Thinge,\n",
            "\n",
            "Bean,\n",
            "\n",
            "\n",
            "I bersisthoughe s t aie m, mo wetoushay thed thare sar d the bes,\n",
            "CHodondind ayof ar d brisesend stho te marens sird t the hery merat derd s te,\n",
            "ARI han de ses o t wristh s mesthar,\n",
            "\n",
            "BRI'st melasinonthout heain ay meear wnth are tinghangilingombs tis se buck:\n",
            "S: th t thang as t t arofthan thiriser beanound\n",
            "Whal tof hastou w mor dondencay buladet t s wh ses witofe hano d arsinde t.\n",
            "T:\n",
            "\n",
            "The f to t at searangou f t a thoro oma myores th tan f d stheres ands,\n",
            "MI t he omy theal to walou hy s owionomalear,\n",
            "Be do oomis, there the bean whe terdan tou d howonoulithallysinte tr avid s f d\n",
            "ICO:\n",
            "\n",
            "Haronthalofotheald anthe o avinghe wavis ore bathoothe bungistig d ha s, dofoure by whesthandee wor, th o toust thinerd thes hom t momy mand, s s m thers fesanowe mathonave s,-----than t he,\n",
            "ANES:\n",
            "WA:\n",
            "METhes, honouplimyor her oull h bu d sthinelou sulds wit winonce w buno areseavell onthe heangour ton hond st st thoo teare thes sofit alloncore t thantinan wigor sheseesousit o or hof fo taincestowor and, h f meen w aly for t save fun, m ave t mby, wnshe dsare, bl f se t atinges are t m, t telingers the thothees d sid f beraree to ben ay so al t\n",
            "Whesthorou my heatheryord,\n",
            "I'\n"
          ]
        }
      ],
      "source": [
        "itos = dataset.itos\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    context = \"O God, O God!\"\n",
        "    tokenized_context = tokenize(context)\n",
        "    # the model should implement a method to generate tokens given a prompt\n",
        "    y = model.generate(tokenized_context, max_new_tokens=6000, temperature=1.0, do_sample=True, top_k=10)\n",
        "    completion = tokens_to_string(y, itos)\n",
        "\n",
        "print(\"Generated text:\")\n",
        "print()\n",
        "print(completion)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}