{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--Cvru1cgwyP"
      },
      "source": [
        "# **Miniproject 2**\n",
        "## **~Large~ Small Language Model**\n",
        "\n",
        "### **Objective**\n",
        "Implement a transformer-based, character-level language model (GPT-like) and train it on the Shakespeare dataset. By the end of this project, you should be able to generate Shakespearean-like text given a seed string.\n",
        "\n",
        "You will probably want to train the model on a GPU. You can use free GPUs on [Google Colab](https://colab.research.google.com/?utm_source=scs-index)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_rT3xwrhieb"
      },
      "source": [
        "### **Dataset**:\n",
        "\n",
        "The Shakespeare dataset contains the complete works of William Shakespeare, including his plays, poems, and sonnets.\n",
        "\n",
        "[**Download link**](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt)\n",
        "\n",
        "In a character-level language model, each character in the input data is mapped to its respective index from a dictionary. The input to the model is in the form (B, N), where B is the batch size and N is the number of tokens for each sequence. The model was tested with B=N=128, but feel free to explore different values.\n",
        "\n",
        "An interface for the dataset class that takes care of tokenization is provided below.\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CharDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Emits batches of characters.\n",
        "\n",
        "    Adapted from \"https://github.com/karpathy/minGPT\".\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, data):\n",
        "\n",
        "        chars = ... # get characters from the input data\n",
        "        self.stoi = { ch:i for i,ch in enumerate(chars) } # map characters to integer indices\n",
        "\n",
        "        ...\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def __len__(self):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # grab a chunk of (block_size + 1) characters from the data\n",
        "        # encode every character to an integer\n",
        "        # return the chunk and the shifted version as tensors\n",
        "        pass\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VV7OAXGRhf_V"
      },
      "source": [
        "### **Requirements**\n",
        "\n",
        "#### **Architecture**\n",
        "\n",
        "Implement the Transformer's decoder-only structure.\n",
        "This includes\n",
        "\n",
        "* input token embeddings\n",
        "* the causal multi-head self-attention mechanism\n",
        "* feed-forward neural networks\n",
        "* positional encodings, residual connections, layer normalizations.\n",
        "\n",
        "The project was tested with $12$ layers, $8$ attention heads, and $768$ embedding dimensions, on a single GPU.\n",
        "\n",
        "The `forward` method for the entire model has the following form:\n",
        "\n",
        "```\n",
        "tok_emb = WTE(idx) # token embeddings\n",
        "pos_emb = WPE(pos) # position embeddings\n",
        "x = Dropout(tok_emb + pos_emb)\n",
        "for Block in Blocks:\n",
        "    x = Block(x)\n",
        "x = Final_LayerNorm(x)\n",
        "logits = LM_Head(x)\n",
        "```\n",
        "\n",
        "The `forward` method for the transformer block has the following form:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "x = x + self.CausalSelfAttn(self.LayerNorm_1(x))\n",
        "out = x + self.MLP(self.LayerNorm_2(x))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### **Training**\n",
        "\n",
        "In a character-level transformer language model, the goal is to predict the next character in a sequence given the previous characters. To train such a model effectively, we use two versions of our data: the input sequence and a shifted version of this sequence, which serves as the target for our predictions.\n",
        "\n",
        "Preprocess the dataset to a character-level representation.\n",
        "Use a sliding window approach for sequence chunks (e.g., window size of $128$ characters).\n",
        "Implement causal masking for the self-attention mechanism.\n",
        "Use the [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) optimizer and the cross-entropy loss.\n",
        "\n",
        "**Optional**:\n",
        "\n",
        "* Implement a learning rate decay strategy\n",
        "* Implement gradient clipping\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#### **Evaluation and Inference**\n",
        "\n",
        "* Monitor the cross-entropy loss. Use a seed string to initialize the model and generate Shakespearean-like text.\n",
        "\n",
        "* In order to generate the characters, at each generation step you can either select the character with the highest probability, or you can sample according to the output distribution.\n",
        "\n",
        "The high-level pseudocode for generation is:\n",
        "\n",
        "```python\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    context = \"O God, O God!\"\n",
        "    tokenized_context = tokenize(context)\n",
        "    # the model should implement a method to generate tokens given a prompt\n",
        "    y = model.generate(tokenized, ...)\n",
        "    completion = tokens_to_string(y)\n",
        "```\n",
        "\n",
        "**Optional**:\n",
        "* Compute the [perplexity](https://medium.com/@priyankads/perplexity-of-language-models-41160427ed72#:~:text=Intuitively%2C%20perplexity%20means%20to%20be,loss%20obtained%20from%20the%20model.) metric for quantitative evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8t88Dcn8JZ8M"
      },
      "source": [
        "### **Example Outputs**\n",
        "\n",
        "The following are my outputs after $6000$ steps of training, with the seed string \"O God, O God!\"\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "O God, O God! neither? unto the base very ears,\n",
        "As damned with it.\n",
        "\n",
        "DUKE OF YORK:\n",
        "Away! Once more, one word.\n",
        "\n",
        "RICHARD:\n",
        "Clove, dear so; and therein my son will be\n",
        "false of woe: if ye seems to be the mother\n",
        "Of gracious order this time when R going kinsperse eyes,\n",
        "What dost bewreck her fairer drying tears.\n",
        "\n",
        "NORTHUMBERLAND:\n",
        "Have you forgot the Duke of Norfolk, get him to\n",
        "again; and and agilic: there is my spirit\n",
        "So maly did must such a marble perfection.\n",
        "\n",
        "ELBOW:\n",
        "Come, bring them with oaths, and so deliver\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0SY7CGAhnkp"
      },
      "source": [
        "### Resources:\n",
        "\n",
        "* Vaswani et al., \"Attention is All You Need\": [link](https://arxiv.org/abs/1706.03762)\n",
        "\n",
        "* Illustrated Transformer by Jay Alammar: [link](https://jalammar.github.io/illustrated-transformer/)\n",
        "\n",
        "* OpenAI GPT-2 Paper: [link](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
        "\n",
        "* Deep Learning Course slides on transformers: [link](https://fleuret.org/dlc/materials/dlc-handout-13-3-transformers.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "aD_GTMlZLgpp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "dZdSRWPmgt-H"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    def __init__(self):\n",
        "        self.block = 128\n",
        "        self.batch = 128\n",
        "        self.epochs = 25\n",
        "        self.num_heads = 8\n",
        "        self.lr = 0.0008\n",
        "        self.embed_dim = 768\n",
        "        self.num_layers = 12\n",
        "        self.dropout = 0.1\n",
        "        self.seed = 42\n",
        "        self.device = torch.device(\"cuda\") #if torch.cuda.is_available() else \"cpu\"\n",
        "        self.data_size = 0\n",
        "        self.vocab_size = 0\n",
        "\n",
        "config = Config()\n",
        "\n",
        "class CharDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Emits batches of tokens.\n",
        "\n",
        "    Adapted from \"https://github.com/karpathy/minGPT\".\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, data):\n",
        "        self.config = config\n",
        "\n",
        "\n",
        "        tokens =  sorted(list(set(data))) # get tokens from the input data\n",
        "        self.stoi =  { ch:i for i,ch in enumerate(tokens) } # map tokens to integer indices\n",
        "        self.itos =  { i:ch for i,ch in enumerate(tokens) } # map integer indices to tokens\n",
        "        self.data = data\n",
        "        self.data_size = len(data)\n",
        "        self.vocab_size = len(tokens)\n",
        "\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        return self.vocab_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) // self.config.block\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # grab a chunk of (block_size + 1) tokens from the data\n",
        "        # encode every token to an integer\n",
        "        # return the chunk and the shifted version as tensors\n",
        "        chunk = self.data[idx * self.config.block : (idx + 1) * self.config.block + 1]\n",
        "        dix = [self.stoi[s] for s in chunk]\n",
        "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M71ZqaobBwk5",
        "outputId": "0feda95f-c14a-4b76-b00a-fc829c695969"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size: 8714\n",
            "Vocab size: 65\n"
          ]
        }
      ],
      "source": [
        "text = open(\"input.txt\").read()\n",
        "dataset = CharDataset(config, text)\n",
        "\n",
        "print(f\"Dataset size: {len(dataset)}\")\n",
        "print(f\"Vocab size: {dataset.get_vocab_size()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "TOnIdrCEZUjQ"
      },
      "outputs": [],
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, dropout):\n",
        "        super(CausalSelfAttention, self).__init__()\n",
        "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len_0 = x.size(0)\n",
        "        # print(\"Sequence Length:\", seq_len)\n",
        "        self.causal_mask = torch.triu(torch.ones(seq_len_0, seq_len_0), diagonal=1).bool().to(x.device)\n",
        "\n",
        "        attn_output, _ = self.attention(x, x, x, attn_mask=self.causal_mask)\n",
        "        return attn_output #self.dropout(attn_output)\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_hidden_dim, dropout):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.ca = CausalSelfAttention(embed_dim, num_heads, dropout)\n",
        "        self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_dim, ff_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(ff_hidden_dim, embed_dim)\n",
        "        )\n",
        "        self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Self-attention with residual connection\n",
        "        x = x + self.ca(self.layer_norm1(x))\n",
        "\n",
        "        # Feed-forward with residual connection\n",
        "        x = x + self.feed_forward(self.layer_norm2(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "class TransformerLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, block_size, dropout=0.1):\n",
        "        super(TransformerLanguageModel, self).__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.position_embedding = nn.Embedding(block_size, embed_dim)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, embed_dim * 4, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.final_layer_norm  = nn.LayerNorm(embed_dim)\n",
        "        self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def forward(self, idx):\n",
        "        # Embedding and position embedding\n",
        "        tok_emb = self.token_embedding(idx)\n",
        "        pos = torch.arange(idx.size(1), device=idx.device).unsqueeze(0)\n",
        "        pos_emb = self.position_embedding(pos)\n",
        "\n",
        "        # Adding positional information\n",
        "        x = self.dropout(tok_emb + pos_emb)\n",
        "\n",
        "        # Pass through transformer blocks\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        # Output layer normalization and language model head\n",
        "        x = self.final_layer_norm(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, tokenized_context, max_new_tokens, temperature=1.0, do_sample=False, top_k=None):\n",
        "        \"\"\"\n",
        "        Generate text from a trained model using token indices.\n",
        "\n",
        "        Parameters:\n",
        "        - tokenized_context: LongTensor of shape (1, t), initial context (sequence of indices).\n",
        "        - max_new_tokens: Number of tokens to generate.\n",
        "        - temperature: Controls randomness in sampling; lower values make predictions more deterministic.\n",
        "        - do_sample: If True, sample from the probability distribution; otherwise, use greedy decoding.\n",
        "        - top_k: If not None, restrict sampling to top-k most probable tokens.\n",
        "\n",
        "        Returns:\n",
        "        - LongTensor of shape (1, t + max_new_tokens) with the generated indices.\n",
        "        \"\"\"\n",
        "        device = next(self.parameters()).device  # Get the model's device\n",
        "        idx = torch.tensor(tokenized_context, dtype=torch.long).unsqueeze(0).to(device)  # Shape (1, t)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Crop the context to the last block_size tokens if necessary\n",
        "            idx_cond = idx if idx.size(1) <= self.block_size else idx[:, -self.block_size:]\n",
        "\n",
        "            # Forward pass to get logits\n",
        "            logits = self(idx_cond)  # Shape (1, seq_len, vocab_size)\n",
        "            logits = logits[:, -1, :] / temperature  # Focus on the last token and apply temperature scaling\n",
        "\n",
        "            # Optionally apply top-k filtering\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, top_k, dim=-1)\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')  # Mask probabilities outside the top-k\n",
        "\n",
        "            # Convert logits to probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            # Decide the next token\n",
        "            if do_sample:\n",
        "                idx_next = torch.multinomial(probs, num_samples=1)  # Sample from probabilities\n",
        "            else:\n",
        "                _, idx_next = torch.topk(probs, k=1, dim=-1)  # Greedy decoding (most probable token)\n",
        "\n",
        "            # Append the predicted token to the sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1)  # Update the sequence with the new token\n",
        "\n",
        "        return  idx.squeeze(0)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsgeCbZhZUjR",
        "outputId": "86e3d01d-9bf1-4d13-c5b0-859c2e3ae20f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25, Loss: 2.9375, Perplexity: 18.8693\n",
            "Epoch 2/25, Loss: 2.4916, Perplexity: 12.0804\n",
            "Epoch 3/25, Loss: 2.4745, Perplexity: 11.8761\n",
            "Epoch 4/25, Loss: 2.4688, Perplexity: 11.8086\n",
            "Epoch 5/25, Loss: 2.4655, Perplexity: 11.7688\n",
            "Epoch 6/25, Loss: 2.4629, Perplexity: 11.7392\n",
            "Epoch 7/25, Loss: 2.4627, Perplexity: 11.7363\n",
            "Epoch 8/25, Loss: 2.4595, Perplexity: 11.6989\n",
            "Epoch 9/25, Loss: 2.4579, Perplexity: 11.6801\n",
            "Epoch 10/25, Loss: 2.4555, Perplexity: 11.6519\n",
            "Epoch 11/25, Loss: 2.4546, Perplexity: 11.6423\n",
            "Epoch 12/25, Loss: 2.4533, Perplexity: 11.6271\n",
            "Epoch 13/25, Loss: 2.4510, Perplexity: 11.6004\n",
            "Epoch 14/25, Loss: 2.4477, Perplexity: 11.5621\n",
            "Epoch 15/25, Loss: 2.4476, Perplexity: 11.5611\n",
            "Epoch 16/25, Loss: 2.4449, Perplexity: 11.5288\n",
            "Epoch 17/25, Loss: 2.4428, Perplexity: 11.5052\n",
            "Epoch 18/25, Loss: 2.4404, Perplexity: 11.4779\n",
            "Epoch 19/25, Loss: 2.4381, Perplexity: 11.4516\n",
            "Epoch 20/25, Loss: 2.4361, Perplexity: 11.4289\n",
            "Epoch 21/25, Loss: 2.4351, Perplexity: 11.4168\n",
            "Epoch 22/25, Loss: 2.4326, Perplexity: 11.3890\n",
            "Epoch 23/25, Loss: 2.4321, Perplexity: 11.3826\n",
            "Epoch 24/25, Loss: 2.4309, Perplexity: 11.3690\n",
            "Epoch 25/25, Loss: 2.4312, Perplexity: 11.3724\n"
          ]
        }
      ],
      "source": [
        "#\n",
        "dataloader = DataLoader(dataset, batch_size=config.batch, shuffle=True)\n",
        "\n",
        "# Initializing the model\n",
        "model = TransformerLanguageModel(\n",
        "    vocab_size=dataset.get_vocab_size(),\n",
        "    embed_dim=config.embed_dim,\n",
        "    num_heads=config.num_heads,\n",
        "    num_layers=config.num_layers,\n",
        "    block_size=config.block,\n",
        "    dropout=config.dropout\n",
        ").to(config.device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "avg_loss = []\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.epochs)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(config.epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for x, y in dataloader:\n",
        "        x = x.to(config.device)\n",
        "        y = y.to(config.device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(x)\n",
        "        loss = loss_fn(logits.view(-1, logits.size(-1)), y.view(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Scheduler step\n",
        "    scheduler.step()\n",
        "\n",
        "    # Average loss and perplexity\n",
        "    avg_epoch_loss = total_loss / len(dataloader)\n",
        "    avg_loss.append(avg_epoch_loss)\n",
        "    perplexity = torch.exp(torch.tensor(avg_epoch_loss))\n",
        "    print(f\"Epoch {epoch + 1}/{config.epochs}, Loss: {avg_epoch_loss:.4f}, Perplexity: {perplexity:.4f}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "Ichb_hisM8wQ",
        "outputId": "7ff87937-9201-4426-9d06-704722ddef93"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#plotting the avg loss\n",
        "plt.plot(avg_loss)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Average Loss')\n",
        "plt.title('Average Loss per Epoch')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "X8BvRnoPenuo"
      },
      "outputs": [],
      "source": [
        "def tokens_to_string(tokens, itos):\n",
        "    \"\"\"\n",
        "    Convert a sequence of tokens to a string.\n",
        "\n",
        "    Args:\n",
        "    - tokens (torch.Tensor): Sequence of tokens.\n",
        "    - itos (dict): Integer-to-string mapping.\n",
        "\n",
        "    Returns:\n",
        "    - String representation of the tokens.\n",
        "    \"\"\"\n",
        "    return ''.join([itos[idx] for idx in tokens.tolist()])\n",
        "\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    Convert a string to a list of token indices.\n",
        "\n",
        "    Parameters:\n",
        "    - text: String to be tokenized.\n",
        "\n",
        "    Returns:\n",
        "    - List of token indices.\n",
        "    \"\"\"\n",
        "    return [dataset.stoi[c] for c in text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "0jOqaelyeXIz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79efc1a0-1125-482e-b504-7bf3411148cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text:\n",
            "\n",
            "O God, O God!\n",
            "Longh us stristhome h s, tthtetherer ts o the d pan in y he as, thoulath me; prl.\n",
            "Whyo s t and myoune ss,\n",
            "LIENGret y the t sheed a heeacour,\n",
            "Ange is onouray ha ane d ce ne he wnode terit:\n",
            "\n",
            "Th'st.\n",
            "Thithon ou aison catoure ifand wiorous the bued halye thy s y wissh wad agou, f y bonthageshenk bul pious herovearesthoncaceeat, t two pillly ill d me he ble theno, he the the s:\n",
            "\n",
            "MPAneavengralind oucotoulopichel sher ws me averd t toupresthen watll t s the ngoncanend sthe ghaspan.\n",
            "\n",
            "F hed:\n",
            "Thime\n",
            "The par ll hond heapase apamy med he I athengove.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "The,\n",
            "ANThard weres hist nowas ar w f tr me th ofur, ss athisesin,\n",
            "\n",
            "I me man lla t alls.\n",
            "I n g hthelisthe,\n",
            "F d plo atold tou ll\n",
            "Toupot thaveshes ce t.\n",
            "FO:\n",
            "\n",
            "Thoulanthin.\n",
            "F asu thethe IORIORDUCARUS:\n",
            "Anghiford thindinendorerixet nce s:\n",
            "HNCl t;\n",
            "ARDUS:\n",
            "\n",
            "Heth I:\n",
            "AUEN:\n",
            "Heaisofus fowoustan bul y wis ootr,\n",
            "\n",
            "Parithave at hung wis h onay me foutrene d\n",
            "LLUCERUCEThen yo me hell tadice forore\n",
            "\n",
            "\n",
            "Wher tin, INorar\n",
            "ARE st wigoineand ld!\n",
            "\n",
            "ANTous her rer s or arthe.\n",
            "Shoug hesth he mbere henge\n",
            "Br allll IES:\n",
            "APESathincose seakere nd maiveance imy he louce.\n",
            "And s tem heathall; serthe:\n",
            "ARI t wase t mo I bour,\n",
            "MPENCast\n",
            "ATis hed cal e n as,\n",
            "\n",
            "I t oouthenge ton ces be nofot sond is\n",
            "NUTotrstoulld alllll me th isee oned llonof are choumer s he ame orty ave cke d d sang s\n",
            "\n",
            "Me y t an t atho g m or t ton l winthode wn whise the hen,\n",
            "Anthaprnd su f g whely wncan thum d laror ins illorel t s d t:\n",
            "Whof m ar t terstovin ce oratralllonso prs h wo nd, g\n",
            "\n",
            "\n",
            "T:\n",
            "Whe.\n",
            "\n",
            "Safoner, t sourcouoreris, I p nou t sthel ble.\n",
            "BUS:\n",
            "Fousivere maroneno dimyous\n",
            "Cam, bealeco fobe wred shat:\n",
            "\n",
            "\n",
            "ARUCO:\n",
            "\n",
            "\n",
            "ASoutour hert bl y,\n",
            "ANSo s, ber allles ththen tichis:\n",
            "Bon.\n",
            "MEONI higone, t handoulllllo es or tor wis pr peer poucourofon ale faver sthout thowisend t se t ch th me mes! dgothat pe t mel t ithe if thr llloond icherey my sune yowour ast, f my s le.\n",
            "I s by wir heinoll s!\n",
            "TO:\n",
            "BRDanot h th ce wos t s athotha asanoud bee t he th,\n",
            "OLUEThieraloust y tr thingeve the ge oth' nde dsthe fous r the, heaf coutle be mathathe ndell mprarode wis the f s, th ist:\n",
            "The m IO:\n",
            "MNI fof oowir alan tarive sthe he al wire gnkean I se ICollam, y,\n",
            "E atisey pe weer d so bbllllos thothere burthe horind Is.\n",
            "I h seromangmase wicer tare, we perese ththagaperenoforthatheralo y t arst yountharu toyororyol sarshoune el sts gerane in l t mathy s d s ancofo home y bone ind sthinono the:\n",
            "TETher lofr f;\n",
            "I akig s, th et, wnes indot:\n",
            "ADUE: orer osthe.\n",
            "ARA:\n",
            "Themy, t tat hik pour ay lo hay shen: mer wivanghe, s:\n",
            "\n",
            "Fe oous,\n",
            "\n",
            "Wand,\n",
            "I he e ma lig.\n",
            "FROusesth faryofoucecancame in the n heisorepreat he lovason rellatrig t petie tanghet cke aine ber t wenowat t sth s thonor t sencr, she e he f s weach s me ou my winy:\n",
            "\n",
            "NEO:\n",
            "AROrs we durs\n",
            "Mainds se f s the ay l s:\n",
            "APandr f stiod llllll'd me onotr ovire henerofrigonite the the, rd ar y, me ar s se f che s nghy ould thisst atiss y wit, allll by chollllillise, s the t y wis hes wimere wey ive tha h t he trstonto housthe cove at anous wis nirer g I cofoualous were fous, realerishe s, hemure, windo hed t nd lll f oure t n hicthay; antourit he tharer boy\n",
            "ADUEShathoomy seange wicord t s the sin thawemast llllentha yot plligroblllend hinor ckighondove d s avece ther al dithonger wighe, liriond\n",
            "\n",
            "I why nt d y:\n",
            "\n",
            "\n",
            "AMENoures his the, four\n",
            "I hath fue d e that y iso se ppreau teer d h wrd ay t ben ed ms ang.\n",
            "SThendonong heisthe ntr ooto re or bllthochathee wangherthan, he t isenced d is dy If wndomero h aved y wash tturavesistono f t wome my, mean, mous he w hois,\n",
            "ENT:\n",
            "\n",
            "O:\n",
            "\n",
            "ARUE be to ld thert\n",
            "\n",
            "Tous overd. st\n",
            "\n",
            "Th.\n",
            "KI s m;\n",
            "\n",
            "ENIS: thenoups st pakeiny man me t re win han che hesin I goureanand s INENou were in me m n th ise wiethano meallisecou heritheree ft, car omy y: ornitore an heinont ckinong le\n",
            "\n",
            "MI r s the wifopelld t se ICoure he pee he.\n",
            "To thathene\n",
            "\n",
            "TICAnold d benin wot grucrng d wis he rantherver I her lidof s hand y,\n",
            "\n",
            "S:\n",
            "IO:\n",
            "\n",
            "S:\n",
            "\n",
            "WAngour tifug s br, wit ay irar ath hed d s nd rthanen.\n",
            "\n",
            "I t oraner hy d ourowit ise ay I the he:\n",
            "\n",
            "\n",
            "\n",
            "Bubrenche be t wheshef cer by, hell hilll anitonou meled coul s s psoof l sen o lllse herer dsh che ds he th hamer he bullllll;\n",
            "ARYe alorur I corifo my wisese en f has ast man haivis astond t ISan d brthopathe erelathe d hes s aine meenato, to s I nce g aknous t llost s de\n",
            "Al pallle h chor couranou mele my,\n",
            "Thate the t tand mig ang tispresth d,\n",
            "The co wicourcheris ond, heang llllar's thener,\n",
            "ICotr:\n",
            "The, bamonomantout me in\n",
            "A:\n",
            "\n",
            "Whis t wil ame thene d ous y lllevy we f t, gowe alvello aghoupat s the theted, y his wo he try, hend des f nchar\n",
            "I gothireantote the anged willd slotr he tadisey mile thee tay youlurats dithe poy d st t aved thout wany owiss tofe myoresownd my at be ropefff woubranknd afof tho wes enghilll than the.\n",
            "MI f ot am maks witst o h g f ind,'ene lll atooornd ll huth thenord wig mesanen s thand be souis bue g byore ng fr and cerd y ily I er mel ment\n",
            "\n",
            "Y wouthore se, chaive the me hele hes?\n",
            "\n",
            "\n",
            "Thed, cthin o enof tagre d; f an: malor thersortrad.\n",
            "ABondy\n",
            "Tour toy:\n",
            "Fond bangedoure g wimes wianchour d:\n",
            "O: ad t s thanowofayofor al t he toulou thend antoumang hee cown tceant brthindillth manerinnd and canocy harand orenges wiess gouclll se mbulld l pe win, stthot yot arise.\n",
            "\n",
            "\n",
            "Foure wiser n thins?\n",
            "\n",
            "Y:\n",
            "\n",
            "Anouf ghiris, thanouan\n",
            "ARou lerof;\n",
            "\n",
            "\n",
            "T:\n",
            "A:\n",
            "CE: towherise d:\n",
            "Monilousot d chat cond ay de, myo wh an by m trin tharthe, ce,\n",
            "Weaseatha shet m o higoure be d sal at bl, gon s susathe ar hifoned athe gorerer heacoy toung maroroul micelayoo d im wowis mer me ode olar her mathinod.\n",
            "Whangouleerve ernoutouche s wibure:\n",
            "ANTo f th men mpos st hithes; tiofoffoous!\n",
            "Win;\n",
            "\n",
            "The.\n",
            "Towindso toth ll wond ter:\n",
            "Yoloure than, ct thee:\n",
            "\n",
            "He ait ay.\n",
            "Loungane th,\n",
            "\n",
            "Ane s h my\n",
            "\n",
            "LENore w stontlllan t, se g:\n",
            "He lll ces histhe sind.\n",
            "Tho bunsor: f t erepardaf herrour s hanoun.\n",
            "\n",
            "Br.\n",
            "AMapot ame bedos routrs t, ly flea le heat nthoulld s thound anthe, asus t athe hy s ou, pl dve:\n",
            "DUENDUThise heseveef mustis, seer youiake he icoue gangur:\n",
            "Basoungus\n",
            "I harardrr t the sthaby;\n",
            "ARe rarard hayoonor sthitheand, t,\n",
            "\n",
            "An t brerthenckigren'\n"
          ]
        }
      ],
      "source": [
        "itos = dataset.itos\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    context = \"O God, O God!\"\n",
        "    tokenized_context = tokenize(context)\n",
        "    y = model.generate(tokenized_context, max_new_tokens=6000, temperature=0.8, do_sample=True, top_k=20)\n",
        "    completion = tokens_to_string(y, itos)\n",
        "\n",
        "print(\"Generated text:\")\n",
        "print()\n",
        "print(completion)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}