{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--Cvru1cgwyP"
      },
      "source": [
        "# **Miniproject 2**\n",
        "## **~Large~ Small Language Model**\n",
        "\n",
        "### **Objective**\n",
        "Implement a transformer-based, character-level language model (GPT-like) and train it on the Shakespeare dataset. By the end of this project, you should be able to generate Shakespearean-like text given a seed string.\n",
        "\n",
        "You will probably want to train the model on a GPU. You can use free GPUs on [Google Colab](https://colab.research.google.com/?utm_source=scs-index)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_rT3xwrhieb"
      },
      "source": [
        "### **Dataset**:\n",
        "\n",
        "The Shakespeare dataset contains the complete works of William Shakespeare, including his plays, poems, and sonnets.\n",
        "\n",
        "[**Download link**](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt)\n",
        "\n",
        "In a character-level language model, each character in the input data is mapped to its respective index from a dictionary. The input to the model is in the form (B, N), where B is the batch size and N is the number of tokens for each sequence. The model was tested with B=N=128, but feel free to explore different values.\n",
        "\n",
        "An interface for the dataset class that takes care of tokenization is provided below.\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CharDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Emits batches of characters.\n",
        "\n",
        "    Adapted from \"https://github.com/karpathy/minGPT\".\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, data):\n",
        "\n",
        "        chars = ... # get characters from the input data\n",
        "        self.stoi = { ch:i for i,ch in enumerate(chars) } # map characters to integer indices\n",
        "\n",
        "        ...\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def __len__(self):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # grab a chunk of (block_size + 1) characters from the data\n",
        "        # encode every character to an integer\n",
        "        # return the chunk and the shifted version as tensors\n",
        "        pass\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VV7OAXGRhf_V"
      },
      "source": [
        "### **Requirements**\n",
        "\n",
        "#### **Architecture**\n",
        "\n",
        "Implement the Transformer's decoder-only structure.\n",
        "This includes\n",
        "\n",
        "* input token embeddings\n",
        "* the causal multi-head self-attention mechanism\n",
        "* feed-forward neural networks\n",
        "* positional encodings, residual connections, layer normalizations.\n",
        "\n",
        "The project was tested with $12$ layers, $8$ attention heads, and $768$ embedding dimensions, on a single GPU.\n",
        "\n",
        "The `forward` method for the entire model has the following form:\n",
        "\n",
        "```\n",
        "tok_emb = WTE(idx) # token embeddings\n",
        "pos_emb = WPE(pos) # position embeddings\n",
        "x = Dropout(tok_emb + pos_emb)\n",
        "for Block in Blocks:\n",
        "    x = Block(x)\n",
        "x = Final_LayerNorm(x)\n",
        "logits = LM_Head(x)\n",
        "```\n",
        "\n",
        "The `forward` method for the transformer block has the following form:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "x = x + self.CausalSelfAttn(self.LayerNorm_1(x))\n",
        "out = x + self.MLP(self.LayerNorm_2(x))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### **Training**\n",
        "\n",
        "In a character-level transformer language model, the goal is to predict the next character in a sequence given the previous characters. To train such a model effectively, we use two versions of our data: the input sequence and a shifted version of this sequence, which serves as the target for our predictions.\n",
        "\n",
        "Preprocess the dataset to a character-level representation.\n",
        "Use a sliding window approach for sequence chunks (e.g., window size of $128$ characters).\n",
        "Implement causal masking for the self-attention mechanism.\n",
        "Use the [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) optimizer and the cross-entropy loss.\n",
        "\n",
        "**Optional**:\n",
        "\n",
        "* Implement a learning rate decay strategy\n",
        "* Implement gradient clipping\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#### **Evaluation and Inference**\n",
        "\n",
        "* Monitor the cross-entropy loss. Use a seed string to initialize the model and generate Shakespearean-like text.\n",
        "\n",
        "* In order to generate the characters, at each generation step you can either select the character with the highest probability, or you can sample according to the output distribution.\n",
        "\n",
        "The high-level pseudocode for generation is:\n",
        "\n",
        "```python\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    context = \"O God, O God!\"\n",
        "    tokenized_context = tokenize(context)\n",
        "    # the model should implement a method to generate tokens given a prompt\n",
        "    y = model.generate(tokenized, ...)\n",
        "    completion = tokens_to_string(y)\n",
        "```\n",
        "\n",
        "**Optional**:\n",
        "* Compute the [perplexity](https://medium.com/@priyankads/perplexity-of-language-models-41160427ed72#:~:text=Intuitively%2C%20perplexity%20means%20to%20be,loss%20obtained%20from%20the%20model.) metric for quantitative evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8t88Dcn8JZ8M"
      },
      "source": [
        "### **Example Outputs**\n",
        "\n",
        "The following are my outputs after $6000$ steps of training, with the seed string \"O God, O God!\"\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "O God, O God! neither? unto the base very ears,\n",
        "As damned with it.\n",
        "\n",
        "DUKE OF YORK:\n",
        "Away! Once more, one word.\n",
        "\n",
        "RICHARD:\n",
        "Clove, dear so; and therein my son will be\n",
        "false of woe: if ye seems to be the mother\n",
        "Of gracious order this time when R going kinsperse eyes,\n",
        "What dost bewreck her fairer drying tears.\n",
        "\n",
        "NORTHUMBERLAND:\n",
        "Have you forgot the Duke of Norfolk, get him to\n",
        "again; and and agilic: there is my spirit\n",
        "So maly did must such a marble perfection.\n",
        "\n",
        "ELBOW:\n",
        "Come, bring them with oaths, and so deliver\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0SY7CGAhnkp"
      },
      "source": [
        "### Resources:\n",
        "\n",
        "* Vaswani et al., \"Attention is All You Need\": [link](https://arxiv.org/abs/1706.03762)\n",
        "\n",
        "* Illustrated Transformer by Jay Alammar: [link](https://jalammar.github.io/illustrated-transformer/)\n",
        "\n",
        "* OpenAI GPT-2 Paper: [link](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
        "\n",
        "* Deep Learning Course slides on transformers: [link](https://fleuret.org/dlc/materials/dlc-handout-13-3-transformers.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "aD_GTMlZLgpp"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "dZdSRWPmgt-H"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    def __init__(self):\n",
        "        self.block = 128\n",
        "        self.batch = 128\n",
        "        self.epochs = 20\n",
        "        self.num_heads = 8\n",
        "        self.lr = 0.00003\n",
        "        self.embed_dim = 768\n",
        "        self.num_layers = 12\n",
        "        self.dropout = 0.2\n",
        "        self.seed = 42\n",
        "        self.device = torch.device(\"cuda\") #if torch.cuda.is_available() else \"cpu\"\n",
        "        self.data_size = 0\n",
        "        self.vocab_size = 0\n",
        "\n",
        "config = Config()\n",
        "\n",
        "class CharDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Emits batches of tokens.\n",
        "\n",
        "    Adapted from \"https://github.com/karpathy/minGPT\".\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, data):\n",
        "        self.config = config\n",
        "\n",
        "\n",
        "        tokens =  sorted(list(set(data))) # get tokens from the input data\n",
        "        self.stoi =  { ch:i for i,ch in enumerate(tokens) } # map tokens to integer indices\n",
        "        self.itos =  { i:ch for i,ch in enumerate(tokens) } # map integer indices to tokens\n",
        "        self.data = data\n",
        "        self.data_size = len(data)\n",
        "        self.vocab_size = len(tokens)\n",
        "\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        return self.vocab_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) // self.config.block\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # grab a chunk of (block_size + 1) tokens from the data\n",
        "        # encode every token to an integer\n",
        "        # return the chunk and the shifted version as tensors\n",
        "        chunk = self.data[idx * self.config.block : (idx + 1) * self.config.block + 1]\n",
        "        dix = [self.stoi[s] for s in chunk]\n",
        "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = open(\"input.txt\").read()\n",
        "dataset = CharDataset(config, text)\n",
        "\n",
        "print(f\"Dataset size: {len(dataset)}\")\n",
        "print(f\"Vocab size: {dataset.get_vocab_size()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M71ZqaobBwk5",
        "outputId": "9cefa57e-a854-426a-ec51-3020f6a76a47"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size: 8714\n",
            "Vocab size: 65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
        "attn_output, attn_output_weights = multihead_attn(query, key,value) -->"
      ],
      "metadata": {
        "id": "1nr2dQPcIMpb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "TOnIdrCEZUjQ"
      },
      "outputs": [],
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, dropout):\n",
        "        super(CausalSelfAttention, self).__init__()\n",
        "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len_0 = x.size(0)\n",
        "        # print(\"Sequence Length:\", seq_len)\n",
        "        self.causal_mask = torch.triu(torch.ones(seq_len_0, seq_len_0), diagonal=1).bool().to(x.device)\n",
        "\n",
        "        attn_output, _ = self.attention(x, x, x, attn_mask=self.causal_mask)\n",
        "        return attn_output #self.dropout(attn_output)\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_hidden_dim, dropout):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.ca = CausalSelfAttention(embed_dim, num_heads, dropout)\n",
        "        self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_dim, ff_hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ff_hidden_dim, embed_dim)\n",
        "        )\n",
        "        self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Self-attention with residual connection\n",
        "        x = x + self.ca(self.layer_norm1(x))\n",
        "\n",
        "        # Feed-forward with residual connection\n",
        "        x = x + self.feed_forward(self.layer_norm2(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "class TransformerLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, block_size, dropout=0.1):\n",
        "        super(TransformerLanguageModel, self).__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.position_embedding = nn.Embedding(block_size, embed_dim)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, embed_dim * 4, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.final_layer_norm  = nn.LayerNorm(embed_dim)\n",
        "        self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def forward(self, idx):\n",
        "        # Embedding and position embedding\n",
        "        tok_emb = self.token_embedding(idx)\n",
        "        pos = torch.arange(idx.size(1), device=idx.device).unsqueeze(0)\n",
        "        pos_emb = self.position_embedding(pos)\n",
        "\n",
        "        # Adding positional information\n",
        "        x = self.dropout(tok_emb + pos_emb)\n",
        "\n",
        "        # Pass through transformer blocks\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        # Output layer normalization and language model head\n",
        "        x = self.final_layer_norm(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def generate(self, tokenized_context, max_len, temperature=0.8,  sampling=False):\n",
        "        \"\"\"\n",
        "        Generate text from a trained model using successive blocks of size `block_size`.\n",
        "\n",
        "        Parameters:\n",
        "        - context: Seed string for generation.\n",
        "        - max_len: Total number of tokens to generate.\n",
        "        - sampling: If True, sample the next token based on probabilities. Otherwise, use the highest probability.\n",
        "\n",
        "        Returns:\n",
        "        - Generated indices\n",
        "        \"\"\"\n",
        "        device = next(self.parameters()).device\n",
        "        generated = tokenized_context\n",
        "        context_tensor = torch.tensor(tokenized_context, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            # Crop the context to the last `block_size` tokens if it exceeds the limit\n",
        "            if context_tensor.size(1) > self.block_size:\n",
        "                context_tensor = context_tensor[:, -self.block_size:]\n",
        "\n",
        "            # Forward pass to get logits\n",
        "            logits = self(context_tensor)\n",
        "            logits = logits[:, -1, :]  # Take logits for the last token only\n",
        "            logits = logits / temperature  # Apply temperature scaling\n",
        "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "\n",
        "            # Decide the next token\n",
        "            if sampling:\n",
        "                next_token = torch.multinomial(probs, num_samples=1).item()\n",
        "            else:\n",
        "                next_token = torch.argmax(probs).item()\n",
        "\n",
        "            # Add the next token to the sequence\n",
        "            generated.append(next_token)\n",
        "            next_token_tensor = torch.tensor([[next_token]], device=device)\n",
        "            context_tensor = torch.cat((context_tensor, next_token_tensor), dim=1)\n",
        "\n",
        "            # Stop if we've generated enough tokens\n",
        "            if context_tensor.size(1) >= max_len:\n",
        "                break\n",
        "\n",
        "        return torch.tensor(generated)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsgeCbZhZUjR",
        "outputId": "6e87c65b-7099-404d-8f85-a3527e4958da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Avg loss: 2.8982\n",
            "Epoch 2/20, Avg loss: 2.5530\n",
            "Epoch 3/20, Avg loss: 2.5086\n",
            "Epoch 4/20, Avg loss: 2.4943\n",
            "Epoch 5/20, Avg loss: 2.4851\n",
            "Epoch 6/20, Avg loss: 2.4801\n",
            "Epoch 7/20, Avg loss: 2.4751\n",
            "Epoch 8/20, Avg loss: 2.4725\n",
            "Epoch 9/20, Avg loss: 2.4695\n",
            "Epoch 10/20, Avg loss: 2.4677\n",
            "Epoch 11/20, Avg loss: 2.4651\n",
            "Epoch 12/20, Avg loss: 2.4648\n",
            "Epoch 13/20, Avg loss: 2.4627\n",
            "Epoch 14/20, Avg loss: 2.4613\n",
            "Epoch 15/20, Avg loss: 2.4597\n",
            "Epoch 16/20, Avg loss: 2.4603\n",
            "Epoch 17/20, Avg loss: 2.4579\n",
            "Epoch 18/20, Avg loss: 2.4577\n",
            "Epoch 19/20, Avg loss: 2.4568\n",
            "Epoch 20/20, Avg loss: 2.4553\n"
          ]
        }
      ],
      "source": [
        "#\n",
        "dataloader = DataLoader(dataset, batch_size=config.batch, shuffle=True)\n",
        "\n",
        "#\n",
        "model = TransformerLanguageModel(\n",
        "    vocab_size=dataset.get_vocab_size(),\n",
        "    embed_dim=config.embed_dim,\n",
        "    num_heads=config.num_heads,\n",
        "    num_layers=config.num_layers,\n",
        "    block_size=config.block,\n",
        "    dropout=config.dropout\n",
        ").to(config.device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(config.epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for x, y in dataloader :\n",
        "\n",
        "        x = x.to(config.device) # Move x to the device\n",
        "        y = y.to(config.device) # Move y to the device\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(x)\n",
        "\n",
        "        #\n",
        "        loss = loss_fn(logits.view(-1, logits.size(-1)), y.view(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        #\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Compute avg loss\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"Epoch {epoch + 1}/{config.epochs}, Avg loss: {avg_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokens_to_string(tokens, itos):\n",
        "    \"\"\"\n",
        "    Convert a sequence of tokens to a string.\n",
        "\n",
        "    Args:\n",
        "    - tokens (torch.Tensor): Sequence of tokens.\n",
        "    - itos (dict): Integer-to-string mapping.\n",
        "\n",
        "    Returns:\n",
        "    - String representation of the tokens.\n",
        "    \"\"\"\n",
        "    return ''.join([itos[idx] for idx in tokens.tolist()])\n",
        "\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    Convert a string to a list of token indices.\n",
        "\n",
        "    Parameters:\n",
        "    - text: String to be tokenized.\n",
        "\n",
        "    Returns:\n",
        "    - List of token indices.\n",
        "    \"\"\"\n",
        "    return [dataset.stoi[c] for c in text]"
      ],
      "metadata": {
        "id": "X8BvRnoPenuo"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "itos = dataset.itos\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    context = \"O God, O God!\"\n",
        "    tokenized_context = tokenize(context)\n",
        "    y = model.generate(tokenized_context, max_len=1000, sampling=True)\n",
        "    completion = tokens_to_string(y, itos)\n",
        "\n",
        "print(\"Generated text:\")\n",
        "print()\n",
        "print(completion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jOqaelyeXIz",
        "outputId": "5250b6c3-d7ac-4fa7-bbab-84085e80df23"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text:\n",
            "\n",
            "O God, O God!\n",
            "Whameclo burere,\n",
            "\n",
            "To ge myoutit tail de:\n",
            "Whowhe t it frer at ail wine\n",
            "\n",
            "Whofr ke hou me inolt hacu s avou hereentheat, tour myomequpathemay, es n:\n",
            "\n",
            "LTh wor Myou hyo nd withouces, ain s minomorsheyorin.\n",
            "Bun ave thad l ar. find ano If the thik fomef malllll y werd at dover the,-fifowinghour ul h shar f he m!-is, s, sulistowisene d mameroped my be r bs nelllisonoteaton t ge\n",
            "Thotr palllave, cagore dwisin:\n",
            "\n",
            "An:\n",
            "Thape ore int soreat whr'su t hent.\n",
            "KERINoucomalalallles!\n",
            "IUCO: hee'The--w t thapuso f hay p t t s me's The\n",
            "Th l youcougore theegou IThereay'd waspr thardong beelllyoure pat.\n",
            "\n",
            "MINom rrd\n",
            "NERIf whe tous We ses Lore gesa,\n",
            "GENACI f in ore? berese paine besen,\n",
            "\n",
            "WABERDWhe fo g we.\n",
            "THe th waray d surmy LO:\n",
            "\n",
            "\n",
            "War hof he ws EThane bame st wave mace lle w ad hiceinathe wioutomoursuthethes ator,\n",
            "A:\n",
            "ARERUShe cheillle merulthe hinat acl t mithof l mind tulofo nd, ben br he t s thel y asean, id S:\n",
            "\n",
            "\n",
            "ARPThe winduldr st ss sofo im nt?\n",
            "WAnoullllld angoures realoulind,\n",
            "\n",
            "Th mitecompllires fee:\n",
            "T:\n",
            "UMaki\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}